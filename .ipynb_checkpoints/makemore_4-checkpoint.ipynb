{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f3cbbc-3aa6-432e-ba94-82b832f5244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623164d9-402f-44a1-b74f-0cb155d7c11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12610ba5-9215-4bd7-a6ae-7fb0abdc8933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59bdd572-4de8-4664-beaf-571a7e9516d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e965a1de-9f30-4b4a-ba2a-d29ba7993868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok biolerplate done, now we get to the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1adddcd4-c316-4908-9318-3b230cee70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8114d924-8281-43ee-9c0a-d99239c07a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "424da90e-d2c9-4b95-b5cd-3e1b0d39049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cee2bc92-5acc-48d2-bcd8-353ae9986faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3200, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e839234-8b8a-43ea-bc21-9f0b8d3a43f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2595f4c42f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3e1cdb8-1d8e-4b6c-a751-4f26deb6b18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnraw.shape, bndiff.shape, bnvar_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95dce79d-ed4d-4682-bc8c-167e583342e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += (2*bndiff) * dbndiff2\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "  for j in range(Xb.shape[1]):\n",
    "    ix = Xb[k,j]\n",
    "    dC[ix] += demb[k,j]\n",
    "      \n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd8c917e-36b9-4f3c-b1d5-4b8be5d759fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3199524879455566 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b3bae4a-a81a-4d85-8a8d-ee070ddb08af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 5.3551048040390015e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f21f523-61c3-4252-a702-83b5dd3648f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "046913ef-16c9-47d8-885a-63809c21e5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0658, 0.0864, 0.0191, 0.0486, 0.0176, 0.0800, 0.0233, 0.0348, 0.0186,\n",
       "        0.0333, 0.0369, 0.0374, 0.0414, 0.0280, 0.0354, 0.0148, 0.0090, 0.0213,\n",
       "        0.0159, 0.0566, 0.0494, 0.0205, 0.0245, 0.0714, 0.0605, 0.0242, 0.0256],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f5ed325-6e3b-4748-8a98-0a50462c8210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0658,  0.0864,  0.0191,  0.0486,  0.0176,  0.0800,  0.0233,  0.0348,\n",
       "        -0.9814,  0.0333,  0.0369,  0.0374,  0.0414,  0.0280,  0.0354,  0.0148,\n",
       "         0.0090,  0.0213,  0.0159,  0.0566,  0.0494,  0.0205,  0.0245,  0.0714,\n",
       "         0.0605,  0.0242,  0.0256], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e88de541-6fb3-437e-b65a-96e99c056956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.4238e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2364662a-fe99-4e42-8db2-d78ca16d6a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x259605d0e10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATMAAAFgCAYAAADXQp4HAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIPlJREFUeJzt3X1sV9X9wPGjyENLaWt56MPaIgWkPG8DxAZlKAxkCQFhCUyTwUIgMCCDzmm6KIpbUgcJogbhH0djIuBIBALLMFCkxK3gQAlPUigWgUBhon1uReT+ck7y7a9foNzPt5y293t4v5IbaXu89/Te+/303Hs/53Mf8DzPUwAQ5R5s7w4AgA0EMwBOIJgBcALBDIATCGYAnEAwA+AEghkAJxDMADjhIRUwN2/eVJcuXVLdunVTDzzwQHt3B0A70jn91dXVKi0tTT344IPRFcx0IMvIyGjvbgAIkAsXLqj09PT2CWZr165Vq1atUuXl5Wr48OHqnXfeUY899pjv/6dHZNrnn3+u4uLi7tq2Y8eOvuurrKwU9bdTp06idtevXxf/Dn5qamp82/j9NQoZOnSoqN2xY8d829geEUvW9+OPP1pb140bN0Trks7kkxwD6bpiYmLEVyg2zkXpPouNjVW2+qU1NDQoG/RnZMyYMaLPVKsEsw8//FDl5uaq9evXq9GjR6s1a9aoSZMmqZKSEtWrVy/RjteBzO8XkAQz6c5vj2AmOcmkwUwagCR9I5iFI5hF/nmSfDYjIfqsqFawevVqNW/ePPW73/1ODRo0yAQ1vbP+/ve/t8bmAMB+MNN/LQ4fPqwmTJjw/xt58EHzdXFx8W3tv//+e1VVVRW2AEC7B7NvvvnGXDIkJyeHfV9/re+f3So/P18lJCQ0Ltz8BxCVeWZ5eXnmJn1o0U8tACBS1h8A9OjRQ3Xo0EFduXIl7Pv665SUlNvad+7c2SwAEKiRmX4qOGLECFVYWBj2BER/nZOTY3tzANB6qRk6LWP27Nlq5MiRJrdMp2bU1taap5sAEDXBbObMmep///ufWr58ubnp/9Of/lTt2rXrtocCfrlCfvlCknyixMRE0fbq6upE7fQltK11SXJ2pPk6paWlbb5Nac6RJEdImtuWnZ3t20bnM9rsv6SdtP/SHDhpO1v9v2k5GVbyObG5X1t1BsDixYvNAgD3xdNMALCBYAbACQQzAE4gmAFwAsEMgBMIZgCcQDAD4ITAlc1uWkrIr/icJKHuu+++a/NCfQ899JC1Qn3S4ozSbeqSS7aK/kkSI6Wk6zp58qRvm6ysLNG6pMm1kiRiaXFJXRlGQpJ4LT1ODwrOoR9++MHqcZIk/UrPbSlGZgCcQDAD4ASCGQAnEMwAOIFgBsAJBDMATiCYAXACwQyAEwhmAJwQ2BkAOjvYL0NYkrWvX7AiIS3PK8lalmZmS8oGSzPLpf2XrE86m8Bm36T9l8yauHjxomhd9fX1bV52urq6WtROcg5J91n//v1925w6dUq0Luk2pZ87W+eixsgMgBMIZgCcQDAD4ASCGQAnEMwAOIFgBsAJBDMATiCYAXBCYJNmhw0b5tumtLTUSvneSEiSIyVllqXrkvZfmqQo6Zs0AVTaTlJqWfp7SpI209PTRev66quvrO0zaXKnzTLo0lLXpwQJsdKy8dLzTJL0ayuxNoSRGQAnEMwAOIFgBsAJBDMATiCYAXACwQyAEwhmAJxAMAPgBIIZACcEdgbAsWPHVLdu3e55PZLs80gyuCWZ0g0NDday2bt06SJal7RUt6TUte1S45KZAtL9L8nGl5bNlpJk2ktLiA8cOFDU7syZM9bO7Q6CdtLZBN9//72oneSzKz1n221k9tprr5mTvOmSnZ1tezMA0Pojs8GDB6s9e/a06KUEANASrRJldPBKSUlpjVUDQNs9ANDX+2lpaSorK0s9//zz6vz583e9Bq+qqgpbAKDdg9no0aNVQUGB2rVrl1q3bp0qKytTTz75ZLPvC8zPz1cJCQmNS0ZGhu0uAbgPPOBJCxm1UEVFherdu7davXq1mjt37h1HZk2fkOiRmQ5oPM20Xz/K9tNM6RMwSQ0v6XGS9E3aL+k+kxyn9niaKdXB4tNMabiw9TRTD4IGDRqkKisrVXx8/F3btvqd+cTERPXoo482W0ixc+fOZgGAQCfN1tTUqLNnz6rU1NTW3hSA+5j1YPbCCy+ooqIide7cOfWf//xHPfvss2aY+5vf/Mb2pgCg9S4zdfa1DlzXrl1TPXv2VE888YQ6cOCA+XckdKa3X7Z3XV2d73qkl7C1tbXW7j9I7yvExMRYq7MvvecnSWA+ceKE1W1K9ofN+1xdu3YVrUs6u0JyD1R6z0zy3grb75qwSbpNyedJci9Vul9bJZht3rzZ9ioBwBcTzQE4gWAGwAkEMwBOIJgBcALBDIATCGYAnEAwA+CEwFZN1MlyfglzkgS++vp60fakSb06GdjWRG1JCeK4uDirSb/Hjx+3Nun7xo0bbV4ePD09vU0naUtJS4hLj6eeBmjLDcFxkh5zaRKr5LMpSQyW7leNkRkAJxDMADiBYAbACQQzAE4gmAFwAsEMgBMIZgCcQDAD4ASCGQAnBHYGgM789cv+lZRjlpaw1q/Es5UBPWDAANG69HsSbJGW15aUKpaSls2WZHFLZkNop0+ftrK9SEiy2aVlv22SzpqoE5SXl+4z6fkjOZ62y34zMgPgBIIZACcQzAA4gWAGwAkEMwBOIJgBcALBDIATCGYAnEAwA+CEwM4A0BnVflnVWVlZvuspKysTb09CkrUsrUEv2aa0X/Hx8aJ2169ft/Y+AZsZ3NLZBBLSbPbOnTtbm/Uh3RdVVVWidjExMb5tqqurrc0UqBe+K0P6rgDJ8ZS8m0D6ngmNkRkAJxDMADiBYAbACQQzAE4gmAFwAsEMgBMIZgCcQDAD4ITAJs3qMtB+paAlJZSlZX6lyYDS8tS2kjElbSJJoJT8ntJ9Jk1olCRtSpJ5pcmYKSkponVduXJF2SJNwJWeP7179/Ztc+LECdG6agVJ0NLzX5qQLPk9JeuKpAR6xCOz/fv3qylTpqi0tDSzoW3btt1Wc3/58uUqNTXVZDFPmDBBnBEPAC0VcTDTUX748OFq7dq1d/z5ypUr1dtvv63Wr1+vDh48qLp27aomTZqkGhoaWtxJALB+mTl58mSz3Ikela1Zs0a9/PLLaurUqeZ777//vkpOTjYjuFmzZkW6OQBo+wcAelJ3eXm5ubQMSUhIUKNHj1bFxcXNvpJKT75tugBAuwYzHcg0PRJrSn8d+tmt8vPzTcALLRkZGTa7BOA+0e6pGXl5eaqysrJxuXDhQnt3CcD9HsxCj8RvfeStv27ucbl+pK1rcTVdAKBdg1mfPn1M0CosLGz8nr4Hpp9q5uTk2NwUANzb08yamhpVWloadtP/yJEjKikpSWVmZqqlS5eqv/71r6p///4muL3yyismJ23atGmRbgoAWi+YHTp0SD311FONX+fm5pr/zp49WxUUFKgXX3zR5KLNnz9fVVRUqCeeeELt2rVLlAXelE7I9cv+lZQqlmbQN5ducqudO3f6tomNjbWWNS4tmy0l2R/SLHVpdrZ+Ym1rXZJ8xXPnzrV5CWjpDAxJOWzt7Nmz1o7TDcFMDem+kM4OkbSTnBeRzLiJOJiNGzfO5JPd7aR8/fXXzQIA983TTACwgWAGwAkEMwBOIJgBcALBDIATCGYAnEAwA+CEwJbNtpUMKE3W/ec//2ktGbC+vl60Ll0lxFbSbHZ2tqhdSUmJtXLYkmRSKWlypGT/d+rUyWqpa0lJb0kCtzRR1Pa+ffjhh33bXLt2zWrSrCQJWpKoK03m1RiZAXACwQyAEwhmAJxAMAPgBIIZACcQzAA4gWAGwAkEMwBOIJgBcEJgZwBIymZHkh0s2Z6tTPW4uDjx+xRslf0+ceKEssXmfpVm2ksz4wcOHOjb5syZM6J11dXVWTs3pOWwpeW1JTMApP3/7rvvrM2asMnWLIEQRmYAnEAwA+AEghkAJxDMADiBYAbACQQzAE4gmAFwAsEMgBMIZgCcENgZALqmul9ddUl9fGk9e2kGtCRTvaGhwVoGdGxsrGhdnudZrbVvsx78I488YuXdBNrJkyetvTdBus8k2f21tbVW3zsgmfkhXdcNwWdAui+kM1Jsnf/Sz6/GyAyAEwhmAJxAMAPgBIIZACcQzAA4gWAGwAkEMwBOIJgBcEJgk2aHDRvmm1R37tw5awmU169ft5bo17VrV2tls20m4ErLMUvXJVVWVmYt6VTSf2lip2RdWn19vW+bLl26iNYlPZ42f88HBcnNNhNwbSfqttrIbP/+/WrKlCkqLS3NnPTbtm0L+/mcOXMa6/eHlmeeecZmnwHg3oOZ/gs6fPhwtXbt2mbb6OB1+fLlxmXTpk2RbgYAWvcyc/LkyWbxG7KmpKREumoACNYDgH379qlevXqpAQMGqIULF6pr167ddeJ2VVVV2AIA7R7M9CXm+++/rwoLC9Xf/vY3VVRUZEZyzd2szM/PVwkJCY1LRkaG7S4BuA9Yf5o5a9asxn8PHTrUPJXs27evGa2NHz/+tvZ5eXkqNze38Ws9MiOgAQhcnllWVpbq0aOHKi0tbfb+Wnx8fNgCAIELZhcvXjT3zFJTU1t7UwDuYxFfZupEz6ajLJ0QeeTIEZWUlGSWFStWqBkzZpinmWfPnlUvvvii6tevn5o0aZLtvgNAowe8CNNw9b2vp5566rbvz549W61bt05NmzZNffHFF6qiosIk1k6cOFH95S9/UcnJyaL163tm+kHA0aNHVbdu3e7aVtL1uLg4q5nZHTp0sFaaWlKCW7K9SLZps4R4ZmamqN3XX39t7feUZMZL94V01oFkRoR01oRfKfhIMuilMwA8wedEOoNBOqNGcpwkbaqrq01WRGVlpe8tqIhHZuPGjbvrzvn4448jXSUA3DMmmgNwAsEMgBMIZgCcQDAD4ASCGQAnEMwAOIFgBsAJBDMATgjsOwBGjhzpm1V94cIFK1n2kWRw26xtLqnNHhsbK1pXXV2dqJ0kO16apX7mzBlr+0yaWS7pmzQzXkoyO0FaG19yzKXHyWbd/usW34Eh/QzYnFmhMTID4ASCGQAnEMwAOIFgBsAJBDMATiCYAXACwQyAEwhmAJwQ2KTZzz77zLdsti6la6scsDS5VpL0KC3brMuD+6mvrxetS5pAKUko1e95kJAm10pI95kkuVNa9luakCzZpjS5U3qeSfatzfPs2l1e1N2SpF/JeZaenu7bJpKq/ozMADiBYAbACQQzAE4gmAFwAsEMgBMIZgCcQDAD4ASCGQAnEMwAOCGwMwB0RrVfVrUkG9l2CWXJNm1mSUszy6Vlp/v16+fb5vTp06J1SX9PSdlpaaa3ZEaEtIS19NyQZNpL94XfrJaQhoYGa/2vrq62NlNGuk1Ju9LSUlHfhw0bJtomIzMATiCYAXACwQyAEwhmAJxAMAPgBIIZACcQzAA4gWAGwAmBTZrVZYP9SgdLEgsj2V5bJ1BKEkCl65K2KykpafNS45KkWemxlJQHl5bNlpYHlyQuS5ObJSW4pe1sJmf/KEyGlW5zyJAhVs5FybkTwsgMgBMiCmb5+flq1KhRZkpGr1691LRp026Lrvov7KJFi1T37t1VXFycmjFjhrpy5YrtfgNAy4NZUVGRCVQHDhxQu3fvNvMBJ06cqGpraxvbLFu2TO3YsUNt2bLFtL906ZKaPn16JJsBgNa9Z7Zr166wrwsKCswI7fDhw2rs2LHm1W/vvfee2rhxo3r66adNmw0bNqiBAweaAPj4449H3kMAaO17ZqH3ViYlJZn/6qCmR2sTJkxobJOdna0yMzNVcXFxszeRq6qqwhYAaLNgpp/qLV26VI0ZM6bxyUV5ebl5kpSYmBjWNjk52fysuftw+iWloSUjI6OlXQJwH2txMNP3zo4fP642b958Tx3Iy8szI7zQcuHChXtaH4D7U4vyzBYvXqx27typ9u/fH/aK9ZSUFJMfU1FRETY6008z9c+ayxuS5A4BgLWRma4GqgPZ1q1b1d69e1WfPn3Cfj5ixAiTfFpYWNj4PZ26cf78eZWTkxPJpgCg9UZm+tJSP6ncvn27yTUL3QfT97piYmLMf+fOnatyc3PNQ4H4+Hi1ZMkSE8gifZL5s5/9zDer+ty5c9bKSUtJMqWlswkkGfS2y2ZLylNLZjlI1yXN7pdmeku2Kc2yl5L0TVqqW+de2podInVTcDwjybSXOHnypJVjKT3HIg5m69atM/8dN25c2Pd1+sWcOXPMv998800z5UEny+oP66RJk9S7774byWYAIGIRBTNJlNTz+tauXWsWAGgrzM0E4ASCGQAnEMwAOIFgBsAJBDMATiCYAXACwQyAEwL7DoCDBw+aWQZ309x8z6akE9el9ewlNdDr6upE69IzJmxlgkvr9ktmMEi3KZ3pICGddSDJ7pe+A0CajS/Z5kMPyT5K0hJXkt9Bmh2fIDjPrl27ZvUdAJKZK5JjLj0vNEZmAJxAMAPgBIIZACcQzAA4gWAGwAkEMwBOIJgBcALBDIATAps0qxMybSRl2iwnrUleviJNwJX0TZo0KClNrUn2qTQB1CZpeXBJMql0XVKS4yQtOy09njbLvXcQ9E26z6TJ2ZJEY5JmAeAOCGYAnEAwA+AEghkAJxDMADiBYAbACQQzAE4gmAFwAsEMgBMCOwNAZ/76Zf9evXrVdz01NTVWM5sl2f0xMTGidUnKU/fv31+0rtLSUlG7GzduWCmzrH377beidpIZBdKMd8kMAOm6pDM1bJUjj2R2hWR90hLWVwWfk8zMTGvrks6okXzmIpkJwcgMgBMIZgCcQDAD4ASCGQAnEMwAOIFgBsAJBDMATiCYAXACwQyAEwI7A0DX2vert19bW2uthrikZrm0nro0y1uyrrNnz1p9h4Eka7yystLa+xCk25TWoJdkxkv3hfQ4Sc6hwYMHi9Z17Ngxa+eG9Nzu1q2btcx+6T6THIO6ujorbVo0MsvPz1ejRo0yO6dXr15q2rRpqqSkJKzNuHHjzInZdFmwYEEkmwGAiEUUzIqKitSiRYvUgQMH1O7du828qYkTJ942Qpo3b566fPly47Jy5crIewYArXWZuWvXrrCvCwoKzAjt8OHDauzYsY3fj42NVSkpKZGsGgDa7wFA6N5KUlJS2Pc/+OAD1aNHDzVkyBCVl5d31+teXbmgqqoqbAGANnsAoG8+Ll26VI0ZM8YErZDnnntO9e7dW6WlpamjR4+ql156ydxX++ijj5q9D7dixYqWdgMA7i2Y6Xtnx48fV59++mnY9+fPn9/476FDh6rU1FQ1fvx481Sub9++t61Hj9xyc3Mbv9Yjs4yMjJZ2C8B9qkXBbPHixWrnzp1q//79Kj09/a5tR48e3Vg88E7BTJKCAQBWg5nOHVmyZInaunWr2rdvn+rTp4/v/3PkyBHzXz1CA4BABDN9ablx40a1fft2k2tWXl7eWGZZl4rWl5L657/61a9U9+7dzT2zZcuWmSedw4YNi6hjOonVL5FVkpgnTcaUJiB27NjRt430IUZ8fLyV0tqRlG3Ozs72baNvH0hIk1Mlx0C6/yXrkh5zybGUJlR/+eWXonXZTA6WJNZKk2ZDn2Vbx0l6PrZbMFu3bl1jYmxTGzZsUHPmzDH12ffs2aPWrFljcs/0va8ZM2aol19+2W6vAeBeLzPvRgcvnVgLAG2NieYAnEAwA+AEghkAJxDMADiBYAbACQQzAE4gmAFwQmDLZutMY79sY0k2tTTLOzMzU9SurKxM2SIpCSzNuJaUpg7NkfXT0NBgNctbMlNA2n9JO2lp5xs3bojaSdcnoUteSSQmJvq2qaioEK3r22+/tXae6YKstvaZ5LMp3Z7GyAyAEwhmAJxAMAPgBIIZACcQzAA4gWAGwAkEMwBOIJgBcEJgk2YlLzqRJNRJkxQlyaRS0hLhJ0+etJZMKintLE2OlCYaS5NmJe2kJbgl+0Pary5dulhLbpauS3o8a2pqrJXN9gT7tmvXrqJ1Sc8NSUKv5DhJz2uNkRkAJxDMADiBYAbACQQzAE4gmAFwAsEMgBMIZgCcQDAD4ASCGQAnBHYGQH19vW/pXUlms7TksTQDXZJ1ffToUdG6OnXqZG0GQ3x8vKhdenq6b5vTp0+L1iUpWy5lM5vdb+ZIpOXBJb+ntLyzdJ9JZgpIy353EOxbySyHSD5PkhkRkmMpnXGgMTID4ASCGQAnEMwAOIFgBsAJBDMATiCYAXACwQyAEwhmAJxAMAPghMDOABg5cqRvtvRXX31lLTM7JiZG1E6yPklmfyTZ/RLSDO5Tp05Zq1MvzUCXzq6wuc22Js3sl2bQ25wB0CCY6dCtWzerv2d1dXWbvs/BrE/cUim1bt0687IOPXVGLzk5Oepf//pX2E5btGiR6t69u4qLi1MzZsxQV65ciWQTANAiEQUzPa/vjTfeUIcPH1aHDh1STz/9tJo6dao6ceKE+fmyZcvUjh071JYtW1RRUZG6dOmSmj59est6BgAReMC7x2uApKQktWrVKvXrX/9a9ezZU23cuNH8O3RJM3DgQFVcXKwef/xx0fqqqqpUQkKCGY5H62WmdCgeyRDaj/QwSl41J70UsnmZKZ1oLtlmJJOT27r/NieaS29TeIL+6yupIF5m6vXoq8HKykrfYgotfgCgP4ibN29WtbW15nJTj9b0B33ChAmNbbKzs1VmZqYJZnc7IDqANV0AIFIRB7Njx46ZKK7LrCxYsEBt3bpVDRo0SJWXl5sb34mJiWHtk5OTzc+ak5+fb0ZioSUjIyPiXwIAIg5mAwYMUEeOHFEHDx5UCxcuVLNnzxa9mbs5eXl5ZggZWi5cuNDidQG4f0WcmqFHX/369TP/HjFihPrvf/+r3nrrLTVz5kzzKnX9WvamozP9NDMlJaXZ9ekRnrSYHgC0WtKsvqGs73vpwKZvvBYWFjb+rKSkRJ0/f97cUwOAwIzM9CXh5MmTzU19/ZRBP7nct2+f+vjjj839rrlz56rc3FzzhFM/eViyZIkJZNInmbeWnvZL5NMjQT+xsbGi7ekHGRKS5ELpuiRPFqUJrJJ1SZ/aSvZrJH2TPAGTHidpqWubT+YkTwNDVyt+pLdkJMdJ+jS8a9euvm1qamqUTZIn4pIn09Lz2mxT3FIpdfXqVfXb3/5WXb582QQv/chUB7Jf/vKX5udvvvmmOcF1sqwerU2aNEm9++67kWwCAFokomD23nvv+b7EYO3atWYBgLbERHMATiCYAXACwQyAEwhmAJxAMAPgBIIZACcErtJsKEFRksQnSe6UJhZKE11trqs9kmYliYrSpFmbyanS4xTUpFlpCSZJaRzpcZJWF74pODfq6+tVEJNmQ3FAsn/vuZ6ZbRcvXqRyBoAwugCFLg4bVcFM/xXRFWr1tKHQX05d40wHOP0L+RVoCyL63/6i/Xe4X/vveZ4ZzaalpflepQTuMlN3uLkIHHr3QLSi/+0v2n+H+7H/CQkJonY8AADgBIIZACdERTDTxRtfffXVqC3iSP/bX7T/DvTfX+AeAACAsyMzAPBDMAPgBIIZACcQzAA4ISqCmS7D/cgjj5iy3KNHj1afffaZigavvfaamcXQdNFveQ+q/fv3qylTpphsa93Xbdu2hf1cPytavny5Sk1NNS/c0G+vP3PmjIqW/s+ZM+e24/HMM8+ooNAvxB41apSZ/dKrVy81bdo084azW+emLlq0SHXv3t28jFu/b0O/zjFa+j9u3LjbjoF+mfh9Ecw+/PBD88Yn/Vj3888/V8OHDzcvStEvV4kGgwcPNi+ACS2ffvqpCio9QV7v3+be4bBy5Ur19ttvq/Xr15uXQOu3/uhjYXPyd2v2X9PBq+nx2LRpkwqKoqIiE6gOHDigdu/erX744Qc1ceLEsMIFy5YtUzt27FBbtmwx7fXUv+nTp6to6b82b968sGOgzysrvIB77LHHvEWLFjV+/eOPP3ppaWlefn6+F3SvvvqqN3z4cC8a6VNj69atjV/fvHnTS0lJ8VatWtX4vYqKCq9z587epk2bvKD3X5s9e7Y3depUL1pcvXrV/B5FRUWN+7tjx47eli1bGtt8+eWXpk1xcbEX9P5rv/jFL7w//OEPrbK9QI/MdCmaw4cPm8uZpnM39dfFxcUqGujLMH3Zk5WVpZ5//nnzUuRoVFZWpsrLy8OOhZ4zpy/7o+VYaPo9r/oSaMCAAWrhwoXq2rVrKqgqKyvNf/V7aDX9WdCjnabHQN+20O+xLQ7gMbi1/yEffPCB6tGjhxoyZIh5F6+0lJGfwE00b+qbb74xda6Sk5PDvq+/PnXqlAo6/UEvKCgwHxw9nF6xYoV68skn1fHjx0UvEw4SHci0Ox2L0M+CTl9i6kuyPn36qLNnz6o///nP5qXWOhB06NBBBa16zNKlS9WYMWPMh17T+7lTp04qMTEx8Mfg5h36rz333HOqd+/e5g+8ftH3Sy+9ZO6rffTRR24Hs2inPygh+oXJOrjpA/mPf/zDvP0dbWvWrFmN/x46dKg5Jn379jWjtfHjx6sg0fee9B+9IN9jbUn/58+fH3YM9MMkve/1Hxd9LO5FoC8z9VBU/8W89WmN/jolJUVFG/0X9dFHH1WlpaUq2oT2tyvHQtOX/vocC9rxWLx4sdq5c6f65JNPwsph6f2sb71UVFQE+hgsbqb/d6L/wGs2jkGgg5keUo8YMUIVFhaGDV/11zk5OSra6BLA+i+Q/msUbfSlmf7AND0WuuCefqoZjcciVNVY3zMLyvHQzy10INi6davau3ev2edN6c9Cx44dw46BvkTT92FzAnAM/Pp/J0eOHDH/tXIMvIDbvHmzeWJWUFDgnTx50ps/f76XmJjolZeXe0H3xz/+0du3b59XVlbm/fvf//YmTJjg9ejRwzzlCaLq6mrviy++MIs+NVavXm3+/fXXX5ufv/HGG2bfb9++3Tt69Kh5MtinTx+vvr7eC3r/9c9eeOEF89RPH489e/Z4P//5z73+/ft7DQ0NXhAsXLjQS0hIMOfM5cuXG5e6urrGNgsWLPAyMzO9vXv3eocOHfJycnLMEg39Ly0t9V5//XXTb30M9HmUlZXljR071sr2Ax/MtHfeecccwE6dOplUjQMHDnjRYObMmV5qaqrp909+8hPztT6gQfXJJ5+YIHDrolMaQukZr7zyipecnGz+wIwfP94rKSnxoqH/+gM1ceJEr2fPnia9oXfv3t68efMC9UfxTn3Xy4YNGxrb6D8cv//9772HH37Yi42N9Z599lkTMKKh/+fPnzeBKykpyZw//fr18/70pz95lZWVVrZPCSAATgj0PTMAkCKYAXACwQyAEwhmAJxAMAPgBIIZACcQzAA4gWAGwAkEMwBOIJgBcALBDIATCGYAlAv+DygROZxG58ioAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5d63a1b-51d3-4534-9224-932c152e8c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "435e9019-358c-4872-a306-355f07b9606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aae31cb-df3e-4ab1-94bb-f2b923aca38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cffcee4d-8069-4712-a131-8206254813c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7703\n",
      "  10000/ 200000: 2.1652\n",
      "  20000/ 200000: 2.4168\n",
      "  30000/ 200000: 2.4141\n",
      "  40000/ 200000: 1.9976\n",
      "  50000/ 200000: 2.4185\n",
      "  60000/ 200000: 2.3657\n",
      "  70000/ 200000: 1.9857\n",
      "  80000/ 200000: 2.4006\n",
      "  90000/ 200000: 2.0982\n",
      " 100000/ 200000: 1.9957\n",
      " 110000/ 200000: 2.3097\n",
      " 120000/ 200000: 2.0524\n",
      " 130000/ 200000: 2.4730\n",
      " 140000/ 200000: 2.2658\n",
      " 150000/ 200000: 2.1529\n",
      " 160000/ 200000: 1.9818\n",
      " 170000/ 200000: 1.8129\n",
      " 180000/ 200000: 1.9761\n",
      " 190000/ 200000: 1.9181\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "      for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "  #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "314fbfd5-3ba2-4eb7-84d7-e9005a66904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6d19f06-5680-488c-b6b4-331a40709b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59e04222-ef24-476b-88de-6f800bfacae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0706558227539062\n",
      "val 2.10992693901062\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "307e3154-9fc3-4c8b-b244-5c6fd06d381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56cd2d5e-5fe1-4a0d-97b6-f34a84f9eaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "montaymyah.\n",
      "see.\n",
      "mad.\n",
      "ryla.\n",
      "reisantengraegelie.\n",
      "kaitlin.\n",
      "shi.\n",
      "jen.\n",
      "eden.\n",
      "sana.\n",
      "arleigh.\n",
      "malaia.\n",
      "noshubergshira.\n",
      "sten.\n",
      "joselle.\n",
      "jose.\n",
      "casube.\n",
      "geda.\n",
      "jamyle.\n",
      "els.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
